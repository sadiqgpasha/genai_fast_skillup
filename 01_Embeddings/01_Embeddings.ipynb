{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a538da40-8e84-4273-8f8d-227cd849d311",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b134aff9-12c1-4c65-9b6e-654279303102",
   "metadata": {},
   "source": [
    "Embeddings are a way to represent data—especially text—in a numerical format that machine learning models can understand. In NLP, embeddings transform words, phrases, or even entire documents into dense vectors in a continuous vector space. These vectors capture semantic relationships, so words with similar meanings are placed closer together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9e210d-2e01-4cd5-8c69-01f4d335d915",
   "metadata": {},
   "source": [
    "## Common Types of Embeddings in NLP\n",
    "- **One-Hot Encoding**\n",
    "A basic method where each word is represented by a binary vector. It’s simple but doesn’t capture meaning or relationships between words.\n",
    "- **TF-IDF (Term Frequency–Inverse Document Frequency)**\n",
    "Weights words based on how frequently they appear in a document versus across all documents. It’s useful for feature extraction but lacks semantic understanding.\n",
    "- **Word2Vec**\n",
    "A neural embedding technique that learns word associations from large corpora. It comes in two flavors: Skip-gram and CBOW (Continuous Bag of Words). Famous for capturing analogies like king - man + woman ≈ queen.\n",
    "- **GloVe (Global Vectors for Word Representation)**\n",
    "Combines global word co-occurrence statistics with local context to produce word vectors. It’s like a hybrid between Word2Vec and matrix factorization.\n",
    "- **FastText**\n",
    "Developed by Facebook, it improves on Word2Vec by considering subword information (like character n-grams), which helps with rare or misspelled words.\n",
    "- **ELMo (Embeddings from Language Models)**\n",
    "Contextual embeddings that generate different vectors for the same word depending on its usage in a sentence.\n",
    "- **BERT and Transformer-based Embeddings**\n",
    "These are deep contextual embeddings from models like BERT, RoBERTa, and GPT. They understand the full context of a word by looking at both left and right surroundings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1641acd-af6d-4195-a403-9cac2541ce07",
   "metadata": {},
   "source": [
    "### Vectorization vs Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caf3dc5-cda5-408e-a765-cc6c7d6c583c",
   "metadata": {},
   "source": [
    "### Vectorization: The Classic Approach\n",
    "Vectorization is a broad term for turning text into numbers. Traditional methods include:\n",
    "\n",
    "- **One-Hot Encoding**: Each word is a binary vector with a single 1. No context or similarity captured.\n",
    "  \n",
    "- **Bag of Words (BoW)**: Counts word frequency but ignores order and semantics.\n",
    "\n",
    "  \n",
    "- **TF-IDF**: Weighs words by importance across documents. Still sparse and context-agnostic.\n",
    "These methods are sparse, high-dimensional, and don’t capture meaning—“cat” and “dog” are just as unrelated as “cat” and “banana.”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f26f3-c33a-4ed6-9f30-edf5d0d03fd5",
   "metadata": {},
   "source": [
    "### Embeddings: The Semantic Revolution\n",
    "\n",
    "Embeddings are dense, low-dimensional vectors learned from data. They capture semantic relationships between words.\n",
    "\n",
    "- **Word2Vec, GloVe, and FastText** learn from co-occurrence patterns.\n",
    "  \n",
    "- Contextual embeddings like **BERT or ELMo** generate different vectors for the same word depending on its context.\n",
    "\n",
    "  \n",
    "For example, “bank” in “river bank” vs. “savings bank” will have different embeddings in BERT, but identical vectors in TF-IDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bbee8e-cee2-4943-8dd0-f18a5f015761",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
