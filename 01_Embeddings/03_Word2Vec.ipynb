{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f303b398-2d41-4e61-88c4-b8217b2f2200",
   "metadata": {},
   "source": [
    "### Word2vec vs Doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a3bc61-08e0-4cfc-b8b3-efab0be10fee",
   "metadata": {},
   "source": [
    "Word2Vec is a neural network-based model that learns dense vector representations (embeddings) of words based on their context in a corpus. It captures semantic relationships—so words like “king” and “queen” end up close in vector space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532c5f48-718e-4b16-9aae-aaa7821d52a8",
   "metadata": {},
   "source": [
    "There are two main architectures:\n",
    "\n",
    "- **CBOW (Continuous Bag of Words)**\n",
    "Predicts a target word from surrounding context words.\n",
    "Example: Given “the cat ___ on the mat”, predict “sat”.\n",
    "\n",
    "- **Skip-Gram**\n",
    "Does the reverse—predicts context words from a target word.\n",
    "Example: Given “sat”, predict “the”, “cat”, “on”, “the”, “mat”.\n",
    "\n",
    "Both are trained using a shallow neural network and optimized using techniques like **negative sampling or hierarchical softmax.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc5973fd-3004-4239-8de5-3c939c518907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'data':\n",
      " [-0.01631741  0.0089938  -0.0082783   0.00164443  0.0169952  -0.0089275\n",
      "  0.00903588 -0.01357163 -0.00709614  0.01879338 -0.00315135  0.00064402\n",
      " -0.00827641 -0.01536737 -0.00301485  0.00494066 -0.00177148  0.01106917\n",
      " -0.00549089  0.00452204  0.01091461  0.0166924  -0.00290378 -0.01841146\n",
      "  0.00873711  0.00114581  0.01488771 -0.00162303 -0.00527828 -0.01750591\n",
      " -0.00171363  0.00565091  0.01079952  0.01410548 -0.01140531  0.00371814\n",
      "  0.01218113 -0.00959919 -0.00621546  0.01359666  0.00326131  0.00038279\n",
      "  0.00695176  0.00043307  0.01924443  0.01012374 -0.01783155 -0.01408546\n",
      "  0.00180391  0.0127816 ]\n",
      "\n",
      "Words similar to 'science': [('fun', 0.16704080998897552), ('a', 0.13203266263008118), ('learning', 0.1267007291316986), ('machine', 0.09984554350376129), ('data', 0.04237872734665871), ('love', 0.040677644312381744), ('of', 0.012417477555572987), ('enjoy', -0.012591077946126461), ('is', -0.014479597099125385), ('deep', -0.0560765340924263)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    [\"i\", \"love\", \"data\", \"science\"],\n",
    "    [\"data\", \"science\", \"is\", \"fun\"],\n",
    "    [\"i\", \"enjoy\", \"machine\", \"learning\"],\n",
    "    [\"deep\", \"learning\", \"is\", \"a\", \"part\", \"of\", \"data\", \"science\"]\n",
    "]\n",
    "\n",
    "# Train Word2Vec - skipgram model\n",
    "model_skipgram = Word2Vec(sentences, \n",
    "                 vector_size=50, \n",
    "                 window=2, \n",
    "                 min_count=1, \n",
    "                 sg=1)  # sg=1 for Skip-Gram, sg=0 for CBOW\n",
    "\n",
    "# Access word vectors\n",
    "print(\"Vector for 'data':\\n\", model_skipgram.wv['data'])\n",
    "\n",
    "# Find similar words\n",
    "print(\"\\nWords similar to 'science':\", model_skipgram.wv.most_similar('science'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7313ec3e-688b-4796-95b2-97c7e00e3b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'data':\n",
      " [-0.01631513  0.00899122 -0.00827763  0.00164529  0.01699386 -0.00892628\n",
      "  0.00903487 -0.01357352 -0.0070978   0.01879499 -0.00315362  0.00064257\n",
      " -0.00827751 -0.01536535 -0.00301715  0.00494141 -0.00177357  0.01106989\n",
      " -0.00548864  0.00452071  0.01091368  0.01669276 -0.00290602 -0.01841386\n",
      "  0.00873944  0.00114542  0.01488611 -0.00162427 -0.00527841 -0.01750569\n",
      " -0.0017135   0.00565139  0.01080079  0.01410554 -0.01140593  0.00371592\n",
      "  0.01217804 -0.00959774 -0.00621279  0.01359432  0.00326069  0.00038156\n",
      "  0.00695011  0.00043386  0.01924171  0.01012283 -0.01783208 -0.01408416\n",
      "  0.00180482  0.0127821 ]\n",
      "\n",
      "Words similar to 'science': [('fun', 0.16704080998897552), ('a', 0.13204070925712585), ('learning', 0.1267007291316986), ('machine', 0.09984554350376129), ('data', 0.04238428175449371), ('love', 0.040677644312381744), ('of', 0.01243622787296772), ('enjoy', -0.012591077946126461), ('is', -0.014479792676866055), ('deep', -0.0560765340924263)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train Word2Vec - CBOW model\n",
    "model_CBOW = Word2Vec(sentences, \n",
    "                 vector_size=50, \n",
    "                 window=2, \n",
    "                 min_count=1, \n",
    "                 sg=0)  # sg=1 for Skip-Gram, sg=0 for CBOW\n",
    "\n",
    "# Access word vectors\n",
    "print(\"Vector for 'data':\\n\", model_CBOW.wv['data'])\n",
    "\n",
    "# Find similar words\n",
    "print(\"\\nWords similar to 'science':\", model_CBOW.wv.most_similar('science'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390b0627-6c75-4353-aa28-71657dd535b9",
   "metadata": {},
   "source": [
    "Youtube : https://www.youtube.com/watch?v=UqRCEmrv1gQ&ab_channel=TheSemicolon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b77d873-41be-4097-aac5-9bc467121f9d",
   "metadata": {},
   "source": [
    "### CBOW vs Skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a301e68-e2e8-43c2-b2f0-da683099c777",
   "metadata": {},
   "source": [
    "|Feature|CBOW|Skip-Gram|\n",
    "|---------|---------|----------|\n",
    "|Objective|Predict target word from context|Predict context words from target|\n",
    "|Input|Surrounding words|Center word|\n",
    "|Output|Center word|Surrounding words|\n",
    "|Training Speed|Faster|Slower (more training examples)|\n",
    "|Performance on Rare Words|Weaker|Stronger|\n",
    "|Best for|large datasets with frequent words|Small datasets on rare word representationsm|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1d4672-a3b3-40b1-a256-2ff8ce216373",
   "metadata": {},
   "source": [
    "##### When to Use What:\n",
    "\n",
    "- Use CBOW when:\n",
    "    - You have a large corpus.\n",
    "    - You care more about frequent words.\n",
    "    - You want faster training.\n",
    "      \n",
    "- Use Skip-Gram when:\n",
    "    - You want to capture rare word semantics.\n",
    "    - You’re working with a smaller dataset.\n",
    "    - You need finer-grained embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb6e2f7-2c9e-488a-9cd2-7415326f6853",
   "metadata": {},
   "source": [
    "##### Advantages & ❌ Disadvantages\n",
    "\n",
    "CBOW\n",
    "\n",
    "- ✅ Faster to train.\n",
    "- ✅ Smooths over noisy data by averaging context.\n",
    "- ❌ Doesn’t perform well with infrequent words.\n",
    "- ❌ May lose nuance due to averaging.\n",
    "\n",
    "Skip-Gram\n",
    "\n",
    "- ✅ Better at capturing semantic relationships.\n",
    "- ✅ Works well with rare words.\n",
    "- ❌ Slower to train.\n",
    "- ❌ Generates more training samples (one per context word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5a0ca8-1029-4c09-8910-5c277a006cba",
   "metadata": {},
   "source": [
    "##### Note:  If you're building a multi-agent RAG pipeline and want fast, general-purpose embeddings, CBOW might be enough. But if you're fine-tuning for nuanced understanding—like rare domain-specific terms—Skip-Gram is your friend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df371580-a0c2-4ce2-a8ef-08918c1d64bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
