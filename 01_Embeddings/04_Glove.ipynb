{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ad05380-aa53-4675-ae36-094d37da1ec6",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c75147-4aec-450f-a7fb-d4bbf3468f8f",
   "metadata": {},
   "source": [
    "GloVe is an unsupervised learning algorithm that generates dense vector representations for words. Unlike Word2Vec, which learns embeddings from local context windows, GloVe leverages global co-occurrence statistics across the entire corpus.\n",
    "\n",
    "The idea is simple but brilliant:\n",
    "“Words that appear in similar contexts tend to have similar meanings.”\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3210e94-0a05-4b2f-9e67-354089ba93f0",
   "metadata": {},
   "source": [
    "### ⚙️ How GloVe Works\n",
    "\n",
    "- **Build a Co-occurrence Matrix**\n",
    "Count how often word i appears in the context of word j across the entire corpus.\n",
    "\n",
    "- **Compute Ratios**\n",
    "GloVe focuses on the ratios of co-occurrence probabilities between word pairs. For example:\n",
    "    - ice and steam both co-occur with solid, gas, water, but in different proportions.\n",
    "    - These ratios help distinguish semantic relationships.\n",
    "\n",
    "- **Train Word Vectors**\n",
    "GloVe minimizes a cost function that ensures the dot product of word vectors approximates the log of co-occurrence counts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b9cd7d-93ce-486b-af14-c35c90492fb2",
   "metadata": {},
   "source": [
    "Let’s say we have a tiny corpus:\n",
    "\n",
    "    \"I like deep learning\"\n",
    "    \"I like NLP\"\n",
    "    \"I enjoy flying\"\n",
    "\n",
    "From this, we build a co-occurrence matrix (window size = 1):\n",
    "\n",
    "\n",
    "||I|like|deep|learning|NLP|enjoy|flying|\n",
    "|--------|------|-------|------|-----|-------|-------|-------|\n",
    "|I|0|2|0|0|1|1|0|\n",
    "|like|2|0|1|0|1|0|0|\n",
    "|deep|0|1|0|1|0|0|0|\n",
    "|learning|0|0|1|0|0|0|0|\n",
    "|NLP|1|1|0|0|0|0|0|\n",
    "|enjoy|1|0|0|0|0|0|1|\n",
    "|flying|0|0|0|0|0|1|0|\n",
    "\n",
    "Now, GloVe tries to learn word vectors such that:\n",
    "\n",
    "$$dot(wordvector_i, wordvector_j) ≈ log(cooccurrence(i, j))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc4759f-c8ac-4e69-b36b-9bb917362fc3",
   "metadata": {},
   "source": [
    "So if “deep” and “learning” co-occur often, their vectors will have a high dot product. If “deep” and “flying” never co-occur, their vectors will be far apart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3ed294-25c7-4bba-aa4c-db84e0075ad4",
   "metadata": {},
   "source": [
    "⚙️ What Makes GloVe Special?\n",
    "\n",
    "Instead of just using raw counts, GloVe focuses on ratios of co-occurrence. For example:\n",
    "- “ice” co-occurs more with “cold” than “steam” does.\n",
    "- “steam” co-occurs more with “hot” than “ice” does.\n",
    "\n",
    "These ratios help GloVe learn that “ice” and “steam” are related but different in temperature context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4ee697-1695-4eac-91de-fea28379d7ba",
   "metadata": {},
   "source": [
    "#### Example for pretrained Glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b491a650-f39e-49cc-9e35-89f794b2f196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'king':\n",
      " [-0.32307  -0.87616   0.21977   0.25268   0.22976   0.7388   -0.37954\n",
      " -0.35307  -0.84369  -1.1113   -0.30266   0.33178  -0.25113   0.30448\n",
      " -0.077491 -0.89815   0.092496 -1.1407   -0.58324   0.66869  -0.23122\n",
      " -0.95855   0.28262  -0.078848  0.75315   0.26584   0.3422   -0.33949\n",
      "  0.95608   0.065641  0.45747   0.39835   0.57965   0.39267  -0.21851\n",
      "  0.58795  -0.55999   0.63368  -0.043983 -0.68731  -0.37841   0.38026\n",
      "  0.61641  -0.88269  -0.12346  -0.37928  -0.38318   0.23868   0.6685\n",
      " -0.43321  -0.11065   0.081723  1.1569    0.78958  -0.21223  -2.3211\n",
      " -0.67806   0.44561   0.65707   0.1045    0.46217   0.19912   0.25802\n",
      "  0.057194  0.53443  -0.43133  -0.34311   0.59789  -0.58417   0.068995\n",
      "  0.23944  -0.85181   0.30379  -0.34177  -0.25746  -0.031101 -0.16285\n",
      "  0.45169  -0.91627   0.64521   0.73281  -0.22752   0.30226   0.044801\n",
      " -0.83741   0.55006  -0.52506  -1.7357    0.4751   -0.70487   0.056939\n",
      " -0.7132    0.089623  0.41394  -1.3363   -0.61915  -0.33089  -0.52881\n",
      "  0.16483  -0.98878 ]\n",
      "\n",
      "Analogy result for 'king - man + woman': ['king', 'queen', 'emperor', 'daughter', 'throne', 'princess', 'prince', 'wife', 'son', 'mother']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load GloVe embeddings (e.g., glove.6B.100d.txt)\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Load embeddings\n",
    "glove_path = './glove.6B/glove.6B.100d.txt'  # Download from https://nlp.stanford.edu/projects/glove/\n",
    "glove = load_glove_embeddings(glove_path)\n",
    "\n",
    "# Example: vector for \"king\"\n",
    "print(\"Vector for 'king':\\n\", glove['king'])\n",
    "\n",
    "# Analogy: king - man + woman ≈ queen\n",
    "def analogy(w1, w2, w3):\n",
    "    vec = glove[w1] - glove[w2] + glove[w3]\n",
    "    return sorted(glove.keys(), key=lambda word: np.dot(glove[word], vec), reverse=True)[:10]\n",
    "\n",
    "print(\"\\nAnalogy result for 'king - man + woman':\", analogy('king', 'man', 'woman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8eaeb6-85bd-463b-8d17-0fd6d695a77b",
   "metadata": {},
   "source": [
    "Ref: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ed3c76-bb4f-46f4-aa5d-9dfe7b562aac",
   "metadata": {},
   "source": [
    "### same example with higher dimensional pre-trained Glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b79ce177-16d2-4cb6-8025-ad94e5dd7e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'king':\n",
      " [ 0.0033901 -0.34614    0.28144    0.48382    0.59469    0.012965\n",
      "  0.53982    0.48233    0.21463   -1.0249    -0.34788   -0.79001\n",
      " -0.15084    0.61374    0.042811   0.19323    0.25462    0.32528\n",
      "  0.05698    0.063253  -0.49439    0.47337   -0.16761    0.045594\n",
      "  0.30451   -0.35416   -0.34583   -0.20118    0.25511    0.091111\n",
      "  0.014651  -0.017541  -0.23854    0.48215   -0.9145    -0.36235\n",
      "  0.34736    0.028639  -0.027065  -0.036481  -0.067391  -0.23452\n",
      " -0.13772    0.33951    0.13415   -0.1342     0.47856   -0.1842\n",
      "  0.10705   -0.45834   -0.36085   -0.22595    0.32881   -0.13643\n",
      "  0.23128    0.34269    0.42344    0.47057    0.479      0.074639\n",
      "  0.3344     0.10714   -0.13289    0.58734    0.38616   -0.52238\n",
      " -0.22028   -0.072322   0.32269    0.44226   -0.037382   0.18324\n",
      "  0.058082   0.26938    0.36202    0.13983    0.016815  -0.34426\n",
      "  0.4827     0.2108     0.75618   -0.13092   -0.025741   0.43391\n",
      "  0.33893   -0.16438    0.26817    0.68774    0.311     -0.2509\n",
      "  0.0027749 -0.39809   -0.43399    0.049531  -0.42686   -0.094679\n",
      "  0.56925    0.28742   -0.015721  -0.059162   0.1912    -0.59814\n",
      "  0.65486   -0.31363    0.16881    0.10862    0.075316   0.34093\n",
      " -0.14706    0.8359     0.39697    0.52358   -0.0096367 -0.14406\n",
      "  0.37783   -0.596     -0.063192  -0.85297   -0.3098    -1.0587\n",
      " -1.025      0.4508    -0.73324   -1.2461    -0.028488   0.20299\n",
      "  0.00259    0.31995    0.35744    0.28533    0.228      0.50956\n",
      " -0.35942    0.32683    0.046264  -0.86896   -0.2707    -0.15454\n",
      " -0.32152    0.31121    0.44134    0.85189    0.21065   -0.13741\n",
      " -0.15359   -0.059722   0.027375   0.23724   -0.39197   -0.66065\n",
      "  0.23587    0.032384  -0.64043    0.55004    0.29597    0.14989\n",
      "  0.46079   -0.26561   -0.1607    -0.36328    1.0782     0.31375\n",
      "  0.1149     0.20248    0.032748   0.41082   -0.082536   0.36606\n",
      "  0.18771    0.75415    0.079648   0.24181   -0.60319   -0.37296\n",
      " -0.047767   0.45008   -0.21135    0.022251  -0.084325   0.18644\n",
      " -0.14682    0.56571   -0.30995    0.17423   -0.41122   -0.84772\n",
      " -0.71114    0.69895   -0.13008   -0.34195   -0.30501   -0.12646\n",
      "  0.29957   -0.43488    0.31935    0.2817    -0.20631   -0.48877\n",
      "  0.34477    0.03907    1.6198    -0.6352    -0.0037675 -0.41271\n",
      "  0.30704   -0.50486    0.036385  -0.046386  -0.12004    0.010029\n",
      " -0.49116    0.041486   0.002979  -0.57694   -0.42088   -0.063218\n",
      "  0.0034244 -0.25093   -0.39689   -0.36984    0.32689    0.01385\n",
      "  0.23634   -0.055199  -0.58453    0.13211    0.50943    0.25198\n",
      " -0.0088309 -0.21273   -0.48423    0.5234    -0.32832   -0.013821\n",
      "  0.15812    0.46696    0.036822  -0.090878   0.18854    0.20794\n",
      " -0.42682    0.59705    0.53109    0.19185   -0.16392    0.064956\n",
      " -0.36009   -0.59882   -0.28134    0.1017     0.02601    0.44298\n",
      " -0.31922   -0.22432    0.7828     0.041307   0.1742     0.27777\n",
      "  0.43792   -0.84324    0.27012   -0.21547    0.52408   -0.19426\n",
      " -0.21878   -0.20713    0.092994  -0.15804    0.28716   -0.11911\n",
      " -0.20688   -0.36482    0.68548   -0.10394   -0.49974   -0.47038\n",
      " -1.2953    -0.46236    0.44467    0.13337    0.88762   -0.26494\n",
      "  0.080676  -0.20625   -0.51232    0.31112    0.062035   0.30302\n",
      " -0.33344   -0.20924   -0.17348   -0.43434   -0.45743   -0.077803\n",
      " -0.33248   -0.078633   0.82182    0.082088  -0.68795    0.30266  ]\n",
      "\n",
      "Analogy result for 'king - man + woman': ['king', 'queen', 'throne', 'princess', 'monarch', 'prince', 'emperor', 'bhumibol', 'daughter', 'royal']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load GloVe embeddings (e.g., glove.6B.100d.txt)\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Load embeddings\n",
    "glove_path = './glove.6B/glove.6B.300d.txt'  # Download from https://nlp.stanford.edu/projects/glove/\n",
    "glove = load_glove_embeddings(glove_path)\n",
    "\n",
    "# Example: vector for \"king\"\n",
    "print(\"Vector for 'king':\\n\", glove['king'])\n",
    "\n",
    "# Analogy: king - man + woman ≈ queen\n",
    "def analogy(w1, w2, w3):\n",
    "    vec = glove[w1] - glove[w2] + glove[w3]\n",
    "    return sorted(glove.keys(), key=lambda word: np.dot(glove[word], vec), reverse=True)[:10]\n",
    "\n",
    "print(\"\\nAnalogy result for 'king - man + woman':\", analogy('king', 'man', 'woman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fe7a3d-310a-4a3b-a933-b95bda9c9923",
   "metadata": {},
   "source": [
    "Notice the new word monarch added in higher dim model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae9697a-3a2f-4aa8-a509-8a08e66cd9ed",
   "metadata": {},
   "source": [
    "### Glove vs word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e79dda-4729-4349-9180-bd0d4cd0e1db",
   "metadata": {},
   "source": [
    "|Feature|Glove|Word2vec|\n",
    "|------|--------|---------|\n",
    "|Context|Global(co-occurence matrix)|Local(context window)|\n",
    "|Training Objective|Matrix factorization|Predictive(CBOW/Skip-Gram)|\n",
    "|Interpretability|More interpretable|Less|\n",
    "|Performance|String on analysis tasks|Strong on similarity tasks|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfcb29f-5c6a-4187-b56b-7bddcfd0b282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
