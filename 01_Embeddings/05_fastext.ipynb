{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6738bf6f-732b-475d-9580-70b10c4a75f2",
   "metadata": {},
   "source": [
    "## Fastext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd21727-0d6b-481f-932d-617afe753a03",
   "metadata": {},
   "source": [
    "FastText is a word embedding model that improves upon Word2Vec by representing words as **bags of character n-grams**. This means it can understand subword information like **prefixes, suffixes, and roots‚Äîmaking it especially powerful for morphologically rich languages and rare or misspelled words**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ebb420-1339-4720-bd78-7fe1a304f6d2",
   "metadata": {},
   "source": [
    "### üß† How FastText Works\n",
    "\n",
    "Instead of learning a single vector per word, FastText:\n",
    "\n",
    "    - Breaks each word into character n-grams (e.g., ‚Äúwhere‚Äù ‚Üí <wh, whe, her, ere, re>).\n",
    "    - Learns embeddings for each n-gram.\n",
    "    - Represents a word as the sum of its n-gram vectors.\n",
    "So even if a word wasn‚Äôt seen during training, FastText can still generate a vector by composing its subword parts!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9ae873-9be1-4611-b10a-a3d79c572fa8",
   "metadata": {},
   "source": [
    "### ‚úÖ Advantages\n",
    "\n",
    "- Handles out-of-vocabulary (OOV) words using subword units.\n",
    "- Better for rare words and morphologically complex languages.\n",
    "- Fast to train and supports both CBOW and Skip-Gram.\n",
    "- Pretrained models available in many languages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eefe2a5-e376-488c-a8a1-cd746dd19453",
   "metadata": {},
   "source": [
    "### ‚ùå Disadvantages\n",
    "- Larger model size due to storing n-gram vectors.\n",
    "- Less effective than contextual models (like BERT) for nuanced meaning.\n",
    "- No context awareness‚Äîstill a static embedding model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a040efd-c2fb-43d3-9088-6b7cb7c19f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AtireBM25Model',\n",
       " 'AuthorTopicModel',\n",
       " 'BackMappingTranslationMatrix',\n",
       " 'CoherenceModel',\n",
       " 'Doc2Vec',\n",
       " 'EnsembleLda',\n",
       " 'FAST_VERSION',\n",
       " 'FastText',\n",
       " 'HdpModel',\n",
       " 'KeyedVectors',\n",
       " 'LdaModel',\n",
       " 'LdaMulticore',\n",
       " 'LdaSeqModel',\n",
       " 'LogEntropyModel',\n",
       " 'LsiModel',\n",
       " 'LuceneBM25Model',\n",
       " 'Nmf',\n",
       " 'NormModel',\n",
       " 'OkapiBM25Model',\n",
       " 'Phrases',\n",
       " 'RpModel',\n",
       " 'TfidfModel',\n",
       " 'TranslationMatrix',\n",
       " 'VocabTransform',\n",
       " 'Word2Vec',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_fasttext_bin',\n",
       " 'atmodel',\n",
       " 'basemodel',\n",
       " 'bm25model',\n",
       " 'callbacks',\n",
       " 'coherencemodel',\n",
       " 'doc2vec',\n",
       " 'doc2vec_corpusfile',\n",
       " 'doc2vec_inner',\n",
       " 'ensemblelda',\n",
       " 'fasttext',\n",
       " 'fasttext_corpusfile',\n",
       " 'fasttext_inner',\n",
       " 'hdpmodel',\n",
       " 'interfaces',\n",
       " 'keyedvectors',\n",
       " 'ldamodel',\n",
       " 'ldamulticore',\n",
       " 'ldaseqmodel',\n",
       " 'logentropy_model',\n",
       " 'lsimodel',\n",
       " 'nmf',\n",
       " 'nmf_pgd',\n",
       " 'normmodel',\n",
       " 'phrases',\n",
       " 'rpmodel',\n",
       " 'tfidfmodel',\n",
       " 'translation_matrix',\n",
       " 'utils',\n",
       " 'word2vec',\n",
       " 'word2vec_corpusfile',\n",
       " 'word2vec_inner']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "dir(gensim.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5def7ab6-1465-4e76-ad0a-40d2fc73d867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'learning':\n",
      " [-0.00161744  0.00030467 -0.00056338 -0.0019255  -0.00086632  0.00120399\n",
      "  0.00256037 -0.00128126 -0.00378282  0.00225328  0.00014133 -0.00116593\n",
      " -0.00028679  0.00124179  0.00103948 -0.00108299  0.00153361 -0.00042023\n",
      " -0.00056507  0.00373649  0.00175249 -0.00037233 -0.00339002  0.00169573\n",
      "  0.00209186 -0.00228598 -0.00193779  0.00256243 -0.00136976  0.00425175\n",
      "  0.00310678  0.00304489 -0.0020575   0.00160803 -0.00127648 -0.00108021\n",
      "  0.00054358  0.00061267  0.00377098  0.00531652  0.00313346  0.00023878\n",
      "  0.00367623  0.00027897 -0.00081464 -0.00064092 -0.00443881 -0.00462794\n",
      " -0.00243396  0.00139452]\n",
      "\n",
      "Vector for 'learnify' (OOV):\n",
      " [-8.0400368e-04  9.2568365e-04  1.0074117e-03 -3.8599859e-03\n",
      "  2.0799220e-04 -5.9383974e-04  1.7541428e-03 -5.6410010e-04\n",
      " -2.3562456e-03  4.5533427e-03  9.7036612e-04  2.2101323e-03\n",
      "  2.9260556e-03  2.3477278e-03  2.5861780e-03 -9.7213913e-04\n",
      " -3.8842298e-04  1.1546517e-03 -7.9165213e-04  2.6887755e-03\n",
      " -2.3300997e-03 -2.8159944e-03 -1.1239576e-03  1.3000141e-03\n",
      "  2.0481574e-03 -1.6485035e-03 -2.1759397e-03  1.7416078e-03\n",
      " -8.5178803e-04  1.7486369e-05 -1.1383241e-04 -3.9508386e-04\n",
      " -4.0577841e-03 -1.7328524e-03 -8.1117288e-04  3.2018234e-03\n",
      "  2.9060936e-03  3.1435207e-04 -2.0287710e-03  3.8420244e-03\n",
      "  3.1239854e-03  4.5439773e-04  3.2634402e-04  2.4109909e-03\n",
      " -1.2862084e-03  4.7313859e-04 -2.2044433e-03  7.7232474e-04\n",
      " -1.5263837e-03  3.1210992e-03]\n",
      "\n",
      "Words similar to 'machine': [('of', 0.24704425036907196), ('cool', 0.14275352656841278), ('fun', 0.08206646889448166), ('language', 0.0480140782892704), ('deep', 0.045292045921087265), ('natural', 0.014593677595257759), ('processing', 0.004752831067889929), ('subset', -0.0018556313589215279), ('a', -0.17340372502803802), ('learning', -0.17958620190620422)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    [\"machine\", \"learning\", \"is\", \"fun\"],\n",
    "    [\"deep\", \"learning\", \"is\", \"a\", \"subset\", \"of\", \"machine\", \"learning\"],\n",
    "    [\"natural\", \"language\", \"processing\", \"is\", \"cool\"]\n",
    "]\n",
    "\n",
    "# Train FastText model\n",
    "model = FastText(sentences, \n",
    "                 vector_size=50, \n",
    "                 window=3, \n",
    "                 min_count=1, \n",
    "                 sg=1)\n",
    "\n",
    "# Get word vector\n",
    "print(\"Vector for 'learning':\\n\", model.wv['learning'])\n",
    "\n",
    "# Handle out-of-vocabulary word\n",
    "print(\"\\nVector for 'learnify' (OOV):\\n\", model.wv['learnify'])\n",
    "\n",
    "# Find similar words\n",
    "print(\"\\nWords similar to 'machine':\", model.wv.most_similar('machine'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca6a90-8eec-4300-a525-c1d2c55d9fc5",
   "metadata": {},
   "source": [
    "### Word2vec vs Fastext\n",
    "\n",
    "|Feature|Word2vec|fastext|\n",
    "|---------|---------|----------|\n",
    "|Unit of representation|Whole words only|Words + Character n-grams (subwords)|\n",
    "|OOV Handling|Cannot handle out-of-vocabulary(OOV) words|Can generate vectors for unseen words using subwords|\n",
    "|Morphology Awareness|Ignores internal structure of words|Captures prefixes, suffixes and roots|\n",
    "|Training objective|Predict context or target word (CBOW/Skip-gram)|Same, but with subword information|\n",
    "|Model Size|Smaller|Slightly larger due to n-gram storage|\n",
    "|Performance on rare words|Weaker|Strongerm|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0053aee0-bc06-47b7-ba03-81dc2c3393df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5fadd8-47a6-4eca-b889-78c65e5d1e17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
