{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21fa9030-ceea-4867-b0fe-92ea924d1510",
   "metadata": {},
   "source": [
    "## BERT Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abef97e-dc6d-49ea-bfc5-aec518fe6bee",
   "metadata": {},
   "source": [
    "### üß† What Are BERT Embeddings?\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) embeddings are contextual word representations generated by the BERT model. Unlike Word2Vec, which gives a single vector per word regardless of context, **BERT generates different vectors for the same word depending on its usage in a sentence.**\n",
    "\n",
    "For example:\n",
    "\n",
    "    - ‚ÄúHe sat by the bank of the river.‚Äù\n",
    "    - ‚ÄúShe went to the bank to deposit money.‚Äù\n",
    "  \n",
    "BERT will produce different embeddings for ‚Äúbank‚Äù in each sentence, because it understands the context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495fafaf-d25e-4710-8409-a542eda37aa9",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è How Do BERT Embeddings Work?\n",
    "\n",
    "- **1. Tokenization**\n",
    "BERT uses WordPiece tokenization, breaking words into subwords (e.g., ‚Äúplaying‚Äù ‚Üí ‚Äúplay‚Äù + ‚Äú##ing‚Äù).\n",
    "\n",
    "Special tokens are added:\n",
    "[CLS] at the beginning (used for Classification tasks)\n",
    "[SEP] to separate sentences\n",
    "\n",
    "- **2. Input Representation**\n",
    "Each token is represented as a sum of:\n",
    "\n",
    "- Token embeddings (from the WordPiece vocabulary)\n",
    "- Segment embeddings (to distinguish sentence A from sentence B)\n",
    "- Position embeddings (to encode word order)\n",
    "\n",
    "So the input to BERT is a matrix of shape:\n",
    "\n",
    "[sequence_length x hidden_size]\n",
    "For BERT-base, hidden_size = 768.\n",
    "\n",
    "- **3. Transformer Layers (Encoder Stack)**\n",
    "BERT uses multi-layer bidirectional transformers to process the entire sentence at once, capturing both left and right context.\n",
    "    -  BERT uses a stack of transformer encoder layers (12 in BERT-base, 24 in BERT-large).\n",
    "    -  Each layer has:\n",
    "        - Multi-head self-attention\n",
    "        - Feed-forward neural network\n",
    "        - Layer normalization and residual connections\n",
    "The self-attention mechanism allows each token to attend to every other token in the sequence, capturing deep contextual relationships.\n",
    "\n",
    "- **4. Contextual Embeddings**\n",
    "\n",
    "    - After passing through all layers, each token now has a contextualized embedding.\n",
    "    - For example, the word ‚Äúbank‚Äù in:\n",
    "        - ‚ÄúHe sat by the river bank‚Äù vs.\n",
    "        - ‚ÄúShe deposited money in the bank‚Äù will have different embeddings\n",
    "\n",
    "- **Output**\n",
    "Each token gets a 768-dimensional vector (for BERT-base). You can also pool these to get a sentence embedding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe3887-f25f-4c53-8c21-46a9ea35796b",
   "metadata": {},
   "source": [
    "Types of BERT Embeddings You Can Extract\n",
    "\n",
    "| Embedding Type | Description | \n",
    "|---------|-----------|\n",
    "| Token-level | Embedding for each token in the input (contextualized) | \n",
    "| [CLS] token | Embedding of the [CLS] token, often used as a sentence-level representation | \n",
    "| Mean pooling | Average of all token embeddings (excluding special tokens) | \n",
    "| Max pooling | Max value across each dimension of token embeddings | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa81d1fb-6f76-466e-b182-bc3be0ac0450",
   "metadata": {},
   "source": [
    "##### Python Code:  Example: Sentence Embedding with Mean Poolin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f0f2346-740a-4142-add9-077488f62390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize input\n",
    "sentence = \"The bank will not be open tomorrow.\"\n",
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "# Get hidden states\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    last_hidden_state = outputs.last_hidden_state  # [1, seq_len, 768]\n",
    "\n",
    "# Mean pooling (excluding [CLS] and [SEP])\n",
    "attention_mask = inputs['attention_mask']\n",
    "mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size())\n",
    "sum_embeddings = torch.sum(last_hidden_state * mask_expanded, 1)\n",
    "sum_mask = torch.clamp(mask_expanded.sum(1), min=1e-9)\n",
    "sentence_embedding = sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9327a956-349b-4853-8053-f88d6f0f41f5",
   "metadata": {},
   "source": [
    "### Why Use BERT Embeddings?\n",
    "- ‚úÖ Context-aware: Understands polysemy and syntax\n",
    "- ‚úÖ Powerful for downstream tasks: Classification, QA, NER, etc.\n",
    "- ‚úÖ Transferable: Pretrained on massive corpora, useful even with little labeled data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8453450b-3d19-45e8-89a9-4d1ca45a69eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d9eec7-20c8-4a05-b990-aeb5a63a885f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ec649d-9a00-41c5-ae2d-259ba3c828bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
