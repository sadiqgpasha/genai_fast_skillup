{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "885cc33c-e666-408b-9f13-8f9edc6b71b2",
   "metadata": {},
   "source": [
    "## Architectures:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928984e-f413-49aa-bd4b-521122ef4f81",
   "metadata": {},
   "source": [
    "### 1. Transformer-Based Architectures\n",
    "\n",
    "\n",
    "These are the backbone of most modern GenAI systems.\n",
    "\n",
    "ðŸ”¹ **Encoder-Only (e.g., BERT, RoBERTa)**\n",
    "\n",
    "- Purpose: Understanding tasks (e.g., classification, NER)\n",
    "- Architecture: Only the encoder stack of the Transformer\n",
    "- Not generative: Cannot generate text directly\n",
    "- Use cases: Embedding generation, sentence classification, question answering (extractive)\n",
    "\n",
    "ðŸ”¹ **Decoder-Only (e.g., GPT, LLaMA, Mistral)**\n",
    "\n",
    "- Purpose: Text generation\n",
    "- Architecture: Only the decoder stack with masked self-attention\n",
    "- Autoregressive: Predicts the next token given previous ones\n",
    "- Use cases: Chatbots, story generation, code completion\n",
    "    \n",
    "ðŸ”¹ **Encoder-Decoder (Seq2Seq) (e.g., T5, BART, FLAN-T5)**\n",
    "\n",
    "- Purpose: Text-to-text tasks (translation, summarization, Q&A)\n",
    "- Architecture: Full Transformer with both encoder and decoder\n",
    "- Flexible: Input is encoded, and decoder generates output conditioned on it\n",
    "- Use cases: Translation, summarization, style transfer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691761c3-add5-48dc-bd79-84d366b8a161",
   "metadata": {},
   "source": [
    "![Architectures](Components_of_Transformer_Architecture-3546166406.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da24f267-973e-4fd6-b917-3ce6faf28551",
   "metadata": {},
   "source": [
    "Ref: https://datasciencedojo.com/blog/transformer-models-types-their-uses/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c737d6-8080-45ec-94b8-6ed349531a09",
   "metadata": {},
   "source": [
    "### ðŸ§  2. Diffusion Models (for Images, Audio, Video)\n",
    "\n",
    "These models generate data by reversing a noise process.\n",
    "    \n",
    "ðŸ”¹ Examples: DALLÂ·E 2, Stable Diffusion, Imagen\n",
    "\n",
    "- Process: Start with noise â†’ denoise step-by-step to generate image\n",
    "- Text-to-image: Often conditioned on text embeddings (from CLIP or T5)\n",
    "- Use cases: Image generation, inpainting, super-resolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcb21e2-8e6b-4402-a151-e49fd9a8e19a",
   "metadata": {},
   "source": [
    "![Diffusion Architecture](Stable_Diffusion_architecture-3150774982.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbf85c8-4130-4d9c-b060-cc4cecd1bf70",
   "metadata": {},
   "source": [
    "Ref: https://jalammar.github.io/illustrated-stable-diffusion/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee059ad1-e69d-4963-871c-96fabc043c48",
   "metadata": {},
   "source": [
    "### 3. Autoencoders & Variants\n",
    "\n",
    "ðŸ”¹ **Variational Autoencoders (VAEs)**\n",
    "- Purpose: Learn latent representations for generation\n",
    "- Probabilistic: Encodes input into a distribution\n",
    "- Use cases: Image generation, anomaly detection\n",
    "  \n",
    "ðŸ”¹ **Denoising Autoencoders**\n",
    "- Trained to reconstruct corrupted input\n",
    "- Used in: Pretraining (e.g., BART corrupts text and learns to reconstruct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9afc9e-dd72-4c7a-8b1b-1e69f7cea49c",
   "metadata": {},
   "source": [
    "Ref:https://pyimagesearch.com/2023/10/02/a-deep-dive-into-variational-autoencoders-with-pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b0181-cb34-4151-8317-aaa9df398fac",
   "metadata": {},
   "source": [
    "![VAE](vae-diagram-1-2048x1126-1442333120.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d87d30-fd48-43aa-93ed-f0f89433e319",
   "metadata": {},
   "source": [
    "### 4. Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "Combines retrieval with generation to improve factual accuracy.\n",
    "\n",
    "- Architecture:\n",
    "    - Encoder retrieves relevant documents (e.g., using vector search)\n",
    "    - Decoder generates output conditioned on retrieved context\n",
    "- Examples: RAG (Facebook), RETRO (DeepMind), Atlas\n",
    "- Use cases: Open-domain QA, chatbots with knowledge grounding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f19d4f9-9d9e-4154-9f3c-5d05c57d7ed4",
   "metadata": {},
   "source": [
    "![RAG](EnterpriseRAG-2925482421.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9834c67b-8b48-4e59-b7d5-7623ed6766e6",
   "metadata": {},
   "source": [
    "Ref: https://medium.com/enterprise-rag/an-introduction-to-rag-and-simple-complex-rag-9c3aa9bd017b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d02b3b-1f08-4453-8fcd-3dcb7d87d734",
   "metadata": {},
   "source": [
    "### ðŸ§® 5. Mixture of Experts (MoE)\n",
    "\n",
    "- Architecture: Multiple expert subnetworks; only a few are activated per input\n",
    "- Scalable: Enables training very large models efficiently\n",
    "- Examples: GLaM (Google), Switch Transformer\n",
    "- Use cases: Efficient large-scale generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3025bf9-e49a-4cfa-9e3d-3955ace53630",
   "metadata": {},
   "source": [
    "![MOE](moe_block-1959552305.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dba39cc-9b6b-41d1-9215-f27055251060",
   "metadata": {},
   "source": [
    "Ref: https://huggingface.co/blog/moe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08537316-abab-47ba-b5af-71500efae370",
   "metadata": {},
   "source": [
    "### ðŸ§  6. Multimodal Architectures\n",
    "\n",
    "Designed to handle multiple input types (text, image, audio).\n",
    "\n",
    "ðŸ”¹ Examples:\n",
    "- CLIP: Connects images and text in a shared embedding space\n",
    "- Flamingo, GPT-4V: Vision-language models\n",
    "- Gemini, Kosmos: Unified models for text, image, and more\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9d2629-2e77-4dec-b7a5-dc079aeefaed",
   "metadata": {},
   "source": [
    "![Multi-modal](multimodal-arch.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eec9dad-e623-49ed-bc8c-ada2e4d4979a",
   "metadata": {},
   "source": [
    "https://slds-lmu.github.io/seminar_multimodal_dl/c02-00-multimodal.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee57a8a-ad7e-4465-9b2e-9ecc4f6dacb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
