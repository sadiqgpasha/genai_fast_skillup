{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c1dd455-1fef-4c98-b75a-5f2227dae34a",
   "metadata": {},
   "source": [
    "### Decoder Only Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b098355-aed6-4450-9b68-65bb336ea818",
   "metadata": {},
   "source": [
    "Decoder-only models are designed for autoregressive generation: predicting the next token in a sequence given all previous tokens. They consist solely of Transformer decoder layers, each performing masked self-attention so that the model can only “see” tokens to the left."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246571b1-18d9-4fe3-9cba-cc89285dfcf5",
   "metadata": {},
   "source": [
    "![Decoder Only](transformer-architectures.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f194bbf-8ad6-47c5-9cbb-f8e323b64612",
   "metadata": {},
   "source": [
    "#### Architecture Details\n",
    "\n",
    "1. **Input Representation**\n",
    "- Token embeddings: Each input token is converted to a dense vector.\n",
    "- Position embeddings: Added to token embeddings to encode ordering.\n",
    "- No segment embeddings are needed for single-sequence generation.\n",
    "\n",
    "2. **Stack of Decoder Layers**\n",
    "\n",
    "Each layer consists of:\n",
    "\n",
    "- Masked Multi-Head Self-Attention\n",
    "    - Queries, keys, and values come from the layer’s input.\n",
    "    - A causal mask prevents tokens from attending to future positions.\n",
    "- Feed-Forward Network\n",
    "    - Two linear transformations with a non-linearity (usually GELU) in between.\n",
    "- Layer Normalization & Residual Connections\n",
    "    - Applied around both the attention sub-layer and the feed-forward sub-layer.\n",
    "- Language Modeling Head\n",
    "    - A final linear layer tied to the token embeddings projects hidden states to vocabulary logits.\n",
    "    - Softmax over logits produces a probability distribution over next tokens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879bd400-a723-4137-9fd2-831dc84b2f1f",
   "metadata": {},
   "source": [
    "#### Advantages\n",
    "- Simplicity\n",
    "    - Only one stack of layers—easier to scale and optimize.\n",
    "- Autoregressive Generation\n",
    "    - Naturally suited for tasks requiring left-to-right decoding (e.g., story and code generation).\n",
    "- Efficient Inference\n",
    "    - Can cache past key/value tensors to avoid re-computing attention for earlier tokens.\n",
    "- Pretraining & Fine-tuning\n",
    "    - Standard language modeling pretraining easily transfers to many generation tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1f8819-f7ab-4487-8a9f-6e52f30f4cd6",
   "metadata": {},
   "source": [
    "**Disadvantages**\n",
    "- No Bidirectional Context\n",
    "    - Cannot use right-to-left context, limiting understanding compared with encoder-only or encoder-decoder models.\n",
    "- Exposure Bias\n",
    "    - Trained on ground-truth prefixes, but during generation it conditions on its own previous predictions.\n",
    "- Limited for Discriminative Tasks\n",
    "    - Less suitable for tasks like classification or extractive QA without adding special heads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7b8019-7d8a-4353-8bd2-d8869fa248d0",
   "metadata": {},
   "source": [
    "### Why and When It’s Used\n",
    "- Text Generation\n",
    "    - Chatbots, story writing, code completion, poetry, dialogue systems.\n",
    "- Autoregressive Modeling\n",
    "    - Any scenario where you predict the next element in a sequence (language, music, DNA).\n",
    "- Fine-Tuning on Generation Tasks\n",
    "    - Given a pretrained decoder-only model, you can fine-tune it on specific generation tasks with prompts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d613f2a3-92fb-4076-9edc-503211d61835",
   "metadata": {},
   "source": [
    "#### Python Code Demo: Text Generation with GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9864059-e37e-42ff-aedd-f6fd492eed96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgp/miniconda3/envs/dsenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a distant future, humanity has colonized Mars. The first team to land discovers that there is an artificial moon of Jupiter in the vicinity. Marsites are able to identify the moon by looking at a map of its orbit. They then go on to discover that there is an artificial moon nearby, and that it is the first moon in a massive solar system.\n",
      "\n",
      "In a distant future, humanity has colonized Mars. The first team to land discovers that there is an artificial moon of Jupiter\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# 1) Load pretrained GPT-2 tokenizer & model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model     = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "# 2) Prepare prompt\n",
    "prompt_text = (\n",
    "    \"In a distant future, humanity has colonized Mars. \"\n",
    "    \"The first team to land discovers\"\n",
    ")\n",
    "input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 3) Generate text\n",
    "# - max_length: total length including prompt\n",
    "# - do_sample, top_k, top_p: sampling parameters for more creative output\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.8,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "\n",
    "# 4) Decode and print\n",
    "generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb159fb3-4fcd-44b5-99f7-cbfa592ca5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a dialog for TOny Stark when he encounters Thanos or Hawkeye\n",
      "\n",
      "Crossover: Marvel Unlimited\n",
      "\n",
      "Sora (unseen) is one of the first characters to appear in Marvel Unlimited, and she is the only character to appear in the comics.\n",
      "\n",
      "Sora has her powers from the comics, but they are not so used in the comics. She does have the ability to speak the same language as the Avengers and the Avengers. She is one of the few characters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2) Prepare prompt\n",
    "prompt_text = (\n",
    "    \"Write a dialog for TOny Stark when he encounters Thanos\"\n",
    ")\n",
    "input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 3) Generate text\n",
    "# - max_length: total length including prompt\n",
    "# - do_sample, top_k, top_p: sampling parameters for more creative output\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.8,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "\n",
    "# 4) Decode and print\n",
    "generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd2f4a5-ff58-4634-8451-114077b645a6",
   "metadata": {},
   "source": [
    "**This snippet shows how a decoder-only model takes a text prompt, applies masked self-attention to generate one token at a time, and uses sampling strategies (top_k, top_p) to produce varied, coherent continuations.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e131bb-83c9-4700-ba12-6b98349d00fa",
   "metadata": {},
   "source": [
    "#### Comparison of Autoregressive Sampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d578f-ab31-42a5-a57e-9ee9699e6997",
   "metadata": {},
   "source": [
    "##### 1. Greedy Search\n",
    "Greedy search selects the single token with the highest probability at each step, producing a deterministic output. It’s easy to implement and very fast but often leads to repetitive or bland text.\n",
    "\n",
    "- Pros\n",
    "    - Very fast and memory-light\n",
    "    - Deterministic (same prompt → same output)\n",
    "- Cons\n",
    "    - Low diversity, can get stuck in loops\n",
    "    - Often produces generic or repetitive phrases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d8c454-0485-4123-80f9-0837f35e5f35",
   "metadata": {},
   "source": [
    "Here’s a minimal example of greedy decoding with a decoder-only model (GPT-2). At each step, we pick the highest-probability token and append it until we hit max_length or the EOS token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89a7eff1-a44f-4ffa-ba1e-9722b90097b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time in a land far away, the sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and the moon was shining. The sun was shining, and\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# 1. Load pretrained GPT-2 and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model     = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "# 2. Encode prompt\n",
    "prompt_text = \"Once upon a time in a land far away,\"\n",
    "input_ids   = tokenizer.encode(prompt_text, return_tensors=\"pt\")\n",
    "\n",
    "# 3. Greedy decoding loop\n",
    "max_new_tokens = 50\n",
    "generated = input_ids\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Forward pass to get logits for the last token\n",
    "        outputs = model(generated)\n",
    "        logits  = outputs.logits   # shape: [1, seq_len, vocab_size]\n",
    "\n",
    "        # Select the token with highest probability (greedy)\n",
    "        next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "\n",
    "        # Append to sequence\n",
    "        generated = torch.cat([generated, next_token], dim=-1)\n",
    "\n",
    "        # Stop if EOS token is generated\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "# 4. Decode and print\n",
    "decoded = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f6acbf-0776-4ab1-b116-bb72b14822cb",
   "metadata": {},
   "source": [
    "**Explanation of key steps:**\n",
    "\n",
    "- We encode the prompt into input_ids.\n",
    "- In each iteration, we pass the entire sequence through the model and extract the logits for the last position.\n",
    "- torch.argmax picks the token with the highest logit (greedy choice).\n",
    "- We append that token and repeat until we reach max_new_tokens or generate the EOS token.\n",
    "This ensures a deterministic, left-to-right generation where the most likely continuation is always chosen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16bf407-10df-4011-92d5-fa7447c8730b",
   "metadata": {},
   "source": [
    "##### 2. Beam Search\n",
    "Beam search keeps the top k candidate sequences (beams) at each generation step, expanding each in parallel. It balances exploration and exploitation, yielding higher-quality outputs than greedy search for many tasks.\n",
    "\n",
    "- Pros\n",
    "    - Higher overall quality and coherence\n",
    "    - Can recover from early mistakes by exploring alternatives\n",
    "- Cons\n",
    "    - Increased computational and memory cost\n",
    "    - If beam width is too large, outputs can become overly safe or generic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d4e7a8-636f-418a-8086-6aed343ecf0d",
   "metadata": {},
   "source": [
    "Here’s how you can use Hugging Face’s generate API to perform beam-search decoding with a GPT-2 (decoder-only) model. This lets you explore multiple high-probability continuations in parallel:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f52e0480-360c-4b8e-b108-06390b715546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Beam 1 ===\n",
      "Space travel is all about human ingenuity to reach vast lengths of space.\n",
      "\n",
      "In the past few years, we've learned a lot about space travel. We've learned a lot about how it works. And we've learned a lot about how it's possible.\n",
      "\n",
      "We've learned a lot\n",
      "\n",
      "=== Beam 2 ===\n",
      "Space travel is all about human ingenuity to reach vast lengths of space.\n",
      "\n",
      "In the past few years, we've learned a lot about space travel. We've learned a lot about how it works. And we've learned a lot about how it works in the real world.\n",
      "\n",
      "We've\n",
      "\n",
      "=== Beam 3 ===\n",
      "Space travel is all about human ingenuity to reach vast lengths of space.\n",
      "\n",
      "In the past few years, we've learned a lot about space travel. We've learned a lot about how it works. And we've learned a lot about how it works for us.\n",
      "\n",
      "We've learned a\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# 1. Load pretrained model & tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model     = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "# 2. Prepare your prompt\n",
    "prompt = \"Space travel is all about human ingenuity to reach vast lengths\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 3. Beam search generation\n",
    "#   - num_beams: how many parallel beams to keep\n",
    "#   - num_return_sequences: how many of the final beams to return\n",
    "#   - early_stopping=True: stop when all beams reach EOS\n",
    "beam_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=60,\n",
    "    num_beams=5,\n",
    "    num_return_sequences=3,\n",
    "    early_stopping=True,\n",
    ")\n",
    "\n",
    "# 4. Decode & display each beam\n",
    "for i, beam in enumerate(beam_outputs):\n",
    "    text = tokenizer.decode(beam, skip_special_tokens=True)\n",
    "    print(f\"\\n=== Beam {i+1} ===\\n{text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d4480c-06ad-4592-9ee2-1707695c9a74",
   "metadata": {},
   "source": [
    "##### Key arguments:\n",
    "\n",
    "- num_beams: number of hypotheses tracked at each step (higher → more exhaustive search, but slower).\n",
    "- num_return_sequences: how many of the top beams you want back.\n",
    "- early_stopping=True: stops generation as soon as all beams have generated an end‐of‐sequence token.\n",
    "\n",
    "Beam search trades off between greedy determinism and full random sampling: you get coherent, high-probability outputs, but with more diversity than pure greedy decoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b643817d-4954-4823-b2a5-fcb83cb61e21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41fa1957-b8b5-44e7-bca2-f108530fef27",
   "metadata": {},
   "source": [
    "##### 3. Nucleus (Top-p) Sampling\n",
    "Nucleus sampling constructs a dynamic shortlist of tokens whose cumulative probability mass ≥ p and samples from it. This ensures that only the most plausible tokens are considered, while still allowing randomness.\n",
    "\n",
    "- Pros\n",
    "    - High diversity and creative outputs\n",
    "    - Avoids extremely low-probability words\n",
    "- Cons\n",
    "    - Quality can fluctuate—occasional incoherence\n",
    "    - More unpredictable and harder to reproduce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eb23b9-8fc6-4124-ac69-f0af8da800aa",
   "metadata": {},
   "source": [
    "Here’s a minimal example showing how to do nucleus (top-p) sampling with a decoder-only model (GPT-2). We set do_sample=True, top_p=0.9 (include only the smallest set of tokens whose cumulative prob ≥ 0.9), and top_k=0 (disable top-k filtering).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aed8803c-fadc-43bf-95eb-2012258da778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sample 1 ===\n",
      "Space travel is all about human ingenuity to reach vast lengths of space, to establish infrastructure, to experiment for an underdeveloped and enslaved space environment. But it's also about making the enterprise seem like if nothing else, at least, it's not. This isn't to say that aliens don't play chess with the company we love. Time-lapse pictures of aliens — and popularized for cinematic and cartoon purposes — have been used for many years, despite numerous efforts.\n",
      "\n",
      "\"The difference\n",
      "\n",
      "=== Sample 2 ===\n",
      "Space travel is all about human ingenuity to reach vast lengths, rapid seas and waves without air and water. Today, many writers — including Laurie Penny, Tim Burton, Margaret Atwood, Robert Zemeckis and George Clooney — are inspired by people with particular hunchback ability to go to places that are impossible to imagine and keep their sanity. Others represent plants, herbs and minerals that are invisible to reality and which have miraculous uses within our own minds. Librarians believe these users are\n",
      "\n",
      "=== Sample 3 ===\n",
      "Space travel is all about human ingenuity to reach vast lengths of the limit. It is essentially a kind of hybrid process whereby elements of every spaceship equipped with brains, artificial intelligence, and the quintessential human need to make each step necessary within a futuristic space. Before I tell you what it's like to create this marvel to look out for, you have to answer the question: which UFO or Civilian or Human Space Program came first? The Aviation Act of 1965 was designed to make it harder for terrorists\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# 1. Load pretrained GPT-2 and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model     = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "\n",
    "# 2. Prepare your prompt\n",
    "prompt = \"Space travel is all about human ingenuity to reach vast lengths\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 3. Nucleus (top-p) sampling generation\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,\n",
    "    do_sample=True,        # enable sampling\n",
    "    top_p=0.9,             # nucleus sampling threshold\n",
    "    top_k=0,               # disable top-k\n",
    "    temperature=1.0,       # control randomness\n",
    "    num_return_sequences=3 # generate 3 distinct samples\n",
    ")\n",
    "\n",
    "# 4. Decode and print each sampled continuation\n",
    "for i, output in enumerate(sample_outputs, 1):\n",
    "    text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    print(f\"=== Sample {i} ===\\n{text}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88febcb-a216-4d93-ad14-af3c51231a2b",
   "metadata": {},
   "source": [
    "##### Explanation of key args:\n",
    "- top_p=0.9 means at each step you sample from the smallest set of tokens whose total probability mass is ≥ 0.9.\n",
    "- top_k=0 turns off fixed-size top-k filtering so only top-p is used.\n",
    "- temperature softens or sharpens the distribution (<1 for less random, >1 for more random).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d751513-9c58-4d64-b945-666bc0e85836",
   "metadata": {},
   "source": [
    "**Trade-offs Summary**\n",
    "\n",
    "| Method | Quality | Diversity | Speed | Complexity | \n",
    "|----------|------------|-----------|----------|-------------|\n",
    "| Greedy Search | Low–Medium | Very Low | Very Fast | O(1) per token | \n",
    "| Beam Search (k) | Medium–High | Low–Medium | Slower (O(k)) | O(k) per token | \n",
    "| Nucleus Sampling | Medium–High | High | Moderate | O(vocab) per token | \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e5c49-4092-4d6a-8e36-ad3755f3eb5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
