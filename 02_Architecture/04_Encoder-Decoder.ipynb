{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41016d7f-6bf2-4675-80bd-f32f2c933547",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86845a52-bad2-45db-a218-859ce59f936e",
   "metadata": {},
   "source": [
    "The encoder–decoder (or sequence-to-sequence) transformer maps an input sequence to an output sequence of potentially different length. It combines a bidirectional encoder that builds rich representations of the source and an autoregressive decoder that generates the target conditioned on both past tokens and encoder outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3071698e-eb1f-4ef7-82db-edc478054a30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "244e2614-071b-40fd-b584-97a475975eba",
   "metadata": {},
   "source": [
    "![Encoder-Decoder](transformer-architectures.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae24595f-08c5-4d43-80d1-58fa55824871",
   "metadata": {},
   "source": [
    "### Components\n",
    "1. **Input Embeddings**\n",
    "- Token embeddings: map vocabulary tokens to vectors.\n",
    "- Positional embeddings: inject order information.\n",
    "- (Optional) Segment embeddings: distinguish multiple inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40f8214-1d7d-4087-b51d-4815c29d2994",
   "metadata": {},
   "source": [
    "2. **Encoder Stack**\n",
    "- Multi-head self-attention\n",
    "- Add & layer normalize\n",
    "- Feed-forward network (two linear layers + activation)\n",
    "- Add & layer normalize\n",
    "All layers process the full input bidirectionally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282318ce-77ae-4439-8606-e4f1aa1bd63a",
   "metadata": {},
   "source": [
    "3. **Decoder Stack**\n",
    "- Masked multi-head self-attention (prevents “peeking” at future tokens)\n",
    "- Add & layer normalize\n",
    "- Multi-head cross-attention (queries from decoder, keys/values from encoder)\n",
    "- Add & layer normalize\n",
    "- Feed-forward network\n",
    "- Add & layer normalize\n",
    "The decoder attends both to its own past outputs and the encoder’s final representations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467963e-e6d0-42c1-bf3c-b53119a0cdea",
   "metadata": {},
   "source": [
    "4. **Cross-Attention Mechanism**\n",
    "At each decoding step, cross-attention lets the decoder query encoder outputs. This aligns source and target, enabling the model to focus on relevant parts of the input when generating each token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188282fc-6d85-4f42-928c-feb60af0efe5",
   "metadata": {},
   "source": [
    "5. **Output Head**\n",
    "A linear layer (tied or untied to token embeddings) projects decoder hidden states to vocabulary logits. A softmax converts logits to probabilities for next-token prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee77822-cec1-45a2-98ed-31c7db1c3f3c",
   "metadata": {},
   "source": [
    "**Applications**\n",
    "  \n",
    "- Machine Translation (e.g., English→German)\n",
    "- Summarization (news, documents)\n",
    "- Question Answering (generate answers from passages)\n",
    "- Paraphrasing & Style Transfer\n",
    "- Code Generation (comment→code, code→docstring)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4040fe-213b-4408-a01e-a35edfd7a833",
   "metadata": {},
   "source": [
    "**When to Use**\n",
    "\n",
    "- Your task requires conditional generation (output depends on an input sequence).\n",
    "- Input and output lengths differ or share little vocabulary overlap.\n",
    "- You want a single model to both understand and generate text in one pass.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aa9527-26da-43bf-a689-7d7eeedce658",
   "metadata": {},
   "source": [
    "## Python Code: Summarization with BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4304aaa1-a8a4-4c26-91ca-3e63bb4fda09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgp/miniconda3/envs/dsenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The Amazon rainforest, known as the 'lungs of the planet', is under threat due to deforestation. Recent studies indicate that unprecedented rates of tree loss have led to shifts in biodiversity and climate patterns in South America. The Amazon is one of the most biodiverse places in the world.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# 1. Load pretrained model & tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model     = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model.eval()\n",
    "\n",
    "# 2. Prepare input text\n",
    "article = \"\"\"\n",
    "The Amazon rainforest, known as the 'lungs of the planet', is under threat due to\n",
    "deforestation. Recent studies indicate that unprecedented rates of tree loss have\n",
    "led to shifts in biodiversity and climate patterns in South America.\n",
    "\"\"\"\n",
    "\n",
    "# 3. Tokenize and encode\n",
    "inputs = tokenizer(\n",
    "    article,\n",
    "    max_length=1024,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# 4. Generate summary\n",
    "summary_ids = model.generate(\n",
    "    inputs.input_ids,\n",
    "    num_beams=4,\n",
    "    length_penalty=2.0,\n",
    "    max_length=120,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# 5. Decode and print\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb24076-9f8a-4a4c-a56a-52ecab43cdc3",
   "metadata": {},
   "source": [
    "This example illustrates how the encoder processes the full article, the decoder attends to it, and a concise summary is produced via beam search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ad0de-e281-46de-aac2-5a6fa3bffcf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
