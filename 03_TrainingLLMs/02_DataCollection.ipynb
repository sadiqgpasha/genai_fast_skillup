{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f34e05f-d5f9-45fe-ae0e-c9674100a382",
   "metadata": {},
   "source": [
    "### Training Collection and Curation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943e5e6-dd43-427f-8b9a-9028940f97e2",
   "metadata": {},
   "source": [
    "### <font color='red'> Data Collection and Curation Pipeline </font> \n",
    "The following Python script demonstrates how to:\n",
    "- Gather text corpora from multiple sources\n",
    "- Enforce simple license checks\n",
    "- Track provenance metadata for each document\n",
    "- Consolidate and save the curated collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f07c5098-6ac4-4b20-8eeb-b728aac67293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgp/miniconda3/envs/dsenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_4035/3042686536.py:39: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for res in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus assembled → corpus.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "import arxiv\n",
    "from github import Github\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load GitHub token (for private/public-domain repo access)\n",
    "load_dotenv()\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "OUTPUT_PATH = \"corpus.jsonl\"\n",
    "ALLOWED_LICENSES = {\"Public domain\", \"CC0-1.0\", \"CC-BY-4.0\"}\n",
    "\n",
    "def save_document(doc):\n",
    "    with open(OUTPUT_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# 1) Replace Common Crawl with Wikipedia (English)\n",
    "def collect_wikipedia(n_samples=500):\n",
    "    # Wikipedia 2022-03-01 English dump\n",
    "    ds = load_dataset(\"wikipedia\", \"20220301.en\", split=f\"train[:{n_samples}]\")\n",
    "    for example in ds:\n",
    "        text = example[\"text\"].strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        save_document({\n",
    "            \"text\": text,\n",
    "            \"source\": \"wikipedia\",\n",
    "            \"url\": f\"https://en.wikipedia.org/wiki?curid={example['id']}\",\n",
    "            \"license\": \"CC-BY-SA-3.0\",\n",
    "            \"fetched_at\": datetime.utcnow().isoformat()\n",
    "        })\n",
    "\n",
    "# 2) arXiv abstracts (assume CC-BY)\n",
    "def collect_arxiv(query=\"machine learning\", max_results=100):\n",
    "    search = arxiv.Search(query=query, max_results=max_results)\n",
    "    for res in search.results():\n",
    "        if \"CC-BY\" not in ALLOWED_LICENSES:\n",
    "            continue\n",
    "        save_document({\n",
    "            \"text\": res.summary or \"\",\n",
    "            \"source\": \"arxiv\",\n",
    "            \"url\": res.pdf_url,\n",
    "            \"license\": \"CC-BY-4.0\",\n",
    "            \"fetched_at\": datetime.utcnow().isoformat(),\n",
    "            \"title\": res.title,\n",
    "            \"authors\": [a.name for a in res.authors]\n",
    "        })\n",
    "\n",
    "# 3) Project Gutenberg (public domain)\n",
    "def collect_gutenberg(book_ids=[1342, 11]):\n",
    "    ds = load_dataset(\"gutenberg\", split=\"train\")\n",
    "    for ex in ds:\n",
    "        if ex[\"id\"] in book_ids:\n",
    "            save_document({\n",
    "                \"text\": ex[\"text\"],\n",
    "                \"source\": \"gutenberg\",\n",
    "                \"url\": f\"https://www.gutenberg.org/ebooks/{ex['id']}\",\n",
    "                \"license\": \"Public domain\",\n",
    "                \"fetched_at\": datetime.utcnow().isoformat(),\n",
    "                \"title\": ex.get(\"title\")\n",
    "            })\n",
    "\n",
    "# 4) GitHub READMEs (public-domain/CC0/CC-BY)\n",
    "def collect_github_repos(user=\"pallets\", max_repos=3):\n",
    "    gh   = Github(GITHUB_TOKEN)\n",
    "    org  = gh.get_user(user)\n",
    "    count = 0\n",
    "    for repo in org.get_repos():\n",
    "        if count >= max_repos:\n",
    "            break\n",
    "        lic = None\n",
    "        try:\n",
    "            lic = repo.get_license().license.spdx_id\n",
    "        except:\n",
    "            pass\n",
    "        if lic not in ALLOWED_LICENSES:\n",
    "            continue\n",
    "        try:\n",
    "            readme = repo.get_readme().decoded_content.decode(\"utf-8\")\n",
    "        except:\n",
    "            continue\n",
    "        save_document({\n",
    "            \"text\": readme,\n",
    "            \"source\": \"github\",\n",
    "            \"url\": repo.html_url,\n",
    "            \"license\": lic,\n",
    "            \"fetched_at\": datetime.utcnow().isoformat(),\n",
    "            \"repo\": repo.full_name\n",
    "        })\n",
    "        count += 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # start fresh\n",
    "    if os.path.exists(OUTPUT_PATH):\n",
    "        os.remove(OUTPUT_PATH)\n",
    "\n",
    "    collect_wikipedia(n_samples=500)\n",
    "    collect_arxiv(query=\"deep learning\", max_results=100)\n",
    "    # collect_gutenberg(book_ids=[1342, 11, 84])\n",
    "    collect_github_repos(user=\"pallets\", max_repos=3)\n",
    "\n",
    "    print(f\"Corpus assembled → {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6ef783-8246-44f3-9aa0-135d61f3c83a",
   "metadata": {},
   "source": [
    "##### How It Works\n",
    "- Common Crawl via Hugging Face: samples raw web text, tags with CC0 license.\n",
    "- arXiv API: pulls paper abstracts/summaries, assumes CC-BY licensing.\n",
    "- Gutenberg: uses public-domain ebook texts.\n",
    "- GitHub: selects repositories with approved licenses, fetches README content.\n",
    "    \n",
    "##### Each document is annotated with:\n",
    "- source: origin of the text\n",
    "- url: link to original\n",
    "- license: license label\n",
    "- fetched_at: timestamp for provenance\n",
    "- (optional) metadata like title, authors\n",
    "All documents are appended to corpus.jsonl for downstream cleaning, deduplication, and training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07a9ce0-6b9b-4414-a434-94a4ee19d887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
