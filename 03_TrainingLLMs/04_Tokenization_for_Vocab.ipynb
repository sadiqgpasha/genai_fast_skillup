{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4bcd41-3705-4b42-bc4b-a380cd365efb",
   "metadata": {},
   "source": [
    "### <font color='red'>  Tokenizer Design and Training</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caec141-ba04-45eb-9546-6d5f7a4ba24e",
   "metadata": {},
   "source": [
    "Tokenization breaks raw text into discrete units (tokens) that a model can process. Different strategies balance vocabulary size, handling of rare words, and efficiency. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc5bc49-e2b3-48d9-b24e-3cb5c5f5568a",
   "metadata": {},
   "source": [
    "Common strategies for subword-level vocab building include:\n",
    "- Byte-Pair Encoding (BPE)\n",
    "- WordPiece\n",
    "- Unigram Language Model (SentencePiece)\n",
    "- Character-level\n",
    "- Whitespace/Regex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2ec1cf-f853-4f38-bc12-6723df3a7f53",
   "metadata": {},
   "source": [
    "#### 1. Byte-Pair Encoding (BPE)\n",
    "\n",
    "- Builds vocabulary by iteratively merging the most frequent pair of symbols (bytes or characters).\n",
    "- Captures common subwords (e.g., “ing”, “tion”).\n",
    "- Used by GPT and RoBERTa.\n",
    "  \n",
    "**Advantages**\n",
    "- Compact vocabulary\n",
    "- Efficient for languages with rich morphology\n",
    "  \n",
    "**Disadvantages**\n",
    "- Greedy merges can miss some patterns\n",
    "- Fixed merge order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1e316e0-66b4-4123-a727-cc1b2c2514de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokens: ['ĠTrans', 'form', 'ers', 'Ġare', 'Ġpowerful', 'Ġmodels', 'Ġfor', 'ĠN', 'LP', '.']\n",
      "IDs:    [3426, 759, 295, 350, 5322, 4668, 287, 347, 17886, 18]\n",
      "Decoded:  Transformers are powerful models for NLP.\n"
     ]
    }
   ],
   "source": [
    "# 1. Install the tokenizers library if you haven't already:\n",
    "#    pip install tokenizers\n",
    "\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "from pathlib import Path\n",
    "\n",
    "# 2. Prepare your training data: a directory of plain .txt files\n",
    "data_dir = Path(\"./\")       # e.g. contains file1.txt, file2.txt, …\n",
    "files = [str(p) for p in data_dir.glob(\"*.jsonl\")]\n",
    "\n",
    "# 3. Initialize a BPE tokenizer\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "# 4. Pre-tokenize on bytes + whitespace\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "\n",
    "# 5. Set up the BPE trainer\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=50_000,              # target vocab size\n",
    "    min_frequency=2,                # ignore subwords that occur <2 times\n",
    "    show_progress=True,\n",
    "    special_tokens=[\n",
    "        \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 6. Train the tokenizer on your files\n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "# 7. Post-processing: add special tokens around sequences\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n",
    "\n",
    "# 8. Set up a decoder for human-readable output\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# 9. Save the trained tokenizer to disk\n",
    "tokenizer.save(\"bpe-tokenizer.json\")\n",
    "\n",
    "# 10. Quick test: encode & decode a sample sentence\n",
    "sample = \"Transformers are powerful models for NLP.\"\n",
    "encoded = tokenizer.encode(sample)\n",
    "\n",
    "print(\"Tokens:\", encoded.tokens)\n",
    "print(\"IDs:   \", encoded.ids)\n",
    "\n",
    "decoded = tokenizer.decode(encoded.ids)\n",
    "print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0abbf7a-c18d-453b-a42e-4ece49932eb2",
   "metadata": {},
   "source": [
    "**Explanation of key steps:**\n",
    "\n",
    "- Data files: point data_dir to a folder of raw text (.txt) files.\n",
    "- models.BPE: initializes a BPE subword model with an unknown token.\n",
    "- ByteLevel pre-tokenizer: splits on byte-level whitespace and punctuation for robustness.\n",
    "- BpeTrainer: merges the most frequent symbol pairs up to vocab_size, ignoring very rare tokens.\n",
    "- post_processor and decoder: ensure encoded sequences retain special-token semantics and decode nicely.\n",
    "- Save: you can later load this JSON into transformers via\n",
    "  PreTrainedTokenizerFast(tokenizer_file=\"bpe-tokenizer.json\").\n",
    "  \n",
    "This pipeline yields a BPE subword vocabulary tuned to your corpus—ideal for LLM pretraining or fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f3cf84-358c-49ab-a0e0-3d6fe42f4c16",
   "metadata": {},
   "source": [
    "#### 2. Wordpiece Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61f58889-1764-4475-9152-45936c8ccfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training corpus\n",
    "corpus = [\n",
    "    \"Hello world!\",\n",
    "    \"Tokenization is fun.\",\n",
    "    \"Transformers are powerful models.\",\n",
    "    \"Natural language processing.\"\n",
    "]\n",
    "\n",
    "# Sentence to encode/decode\n",
    "sample = \"Transformers are powerful models for NLP.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8463f43-6ed1-48a9-b550-39e4c4b42063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "WordPiece Tokens: ['Transformers', 'are', 'powerful', 'models', 'f', '##or', '[UNK]', '.']\n",
      "IDs:    [105, 92, 108, 107, 14, 51, 1, 6]\n",
      "WordPiece Decoded: Transformers are powerful models for.\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n",
    "\n",
    "# Initialize WordPiece tokenizer\n",
    "wp_tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "wp_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# Trainer: learn merges under a likelihood model\n",
    "wp_trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size=500,\n",
    "    min_frequency=1,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    ")\n",
    "wp_tokenizer.train_from_iterator(corpus, trainer=wp_trainer)\n",
    "\n",
    "# Decoder for WordPiece (handles '##' prefix)\n",
    "wp_tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n",
    "\n",
    "# Encode & decode\n",
    "encoded = wp_tokenizer.encode(sample)\n",
    "print(\"\\nWordPiece Tokens:\", encoded.tokens)\n",
    "print(\"IDs:   \", encoded.ids)\n",
    "print(\"WordPiece Decoded:\", wp_tokenizer.decode(encoded.ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ee1c8f-fb37-4340-b684-6a18e844e2c3",
   "metadata": {},
   "source": [
    "#### 3. Unigram Language Model (SentencePiece-style)\n",
    "A probabilistic model that starts from a large seed vocab and prunes under an EM algorithm to maximize data likelihood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e86280-6d96-4e86-91f9-37b49718bdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Unigram Tokens: ['T', 'ra', 'n', 's', 'f', 'or', 'm', 'er', 's', 'a', 'r', 'e', 'p', 'o', 'w', 'er', 'fu', 'l', 'm', 'o', 'd', 'el', 's', 'f', 'or', 'N', 'LP', '.']\n",
      "IDs:    [17, 26, 10, 6, 28, 24, 18, 23, 6, 11, 13, 8, 15, 5, 16, 23, 21, 7, 18, 5, 19, 25, 6, 28, 24, 30, 1, 12]\n",
      "Unigram Decoded: TransformersarepowerfulmodelsforN.\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, decoders\n",
    "\n",
    "# Sample corpus and sentence\n",
    "corpus = [\n",
    "    \"Hello world!\",\n",
    "    \"Tokenization is fun.\",\n",
    "    \"Transformers are powerful models.\",\n",
    "    \"Natural language processing.\"\n",
    "]\n",
    "sample = \"Transformers are powerful models for NLP.\"\n",
    "\n",
    "# 1. Initialize Unigram model (no unk_token here!)\n",
    "uni_model = models.Unigram()\n",
    "uni_tokenizer = Tokenizer(uni_model)\n",
    "\n",
    "# 2. Pre-tokenizer\n",
    "uni_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "# 3. Trainer with unk_token specified\n",
    "uni_trainer = trainers.UnigramTrainer(\n",
    "    vocab_size=500,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    unk_token=\"[UNK]\"  # This is where you define it\n",
    ")\n",
    "\n",
    "# 4. Train the tokenizer\n",
    "uni_tokenizer.train_from_iterator(corpus, trainer=uni_trainer)\n",
    "\n",
    "# 6. Optional: add decoder\n",
    "uni_tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# 7. Encode and decode\n",
    "encoded = uni_tokenizer.encode(sample)\n",
    "print(\"\\nUnigram Tokens:\", encoded.tokens)\n",
    "print(\"IDs:   \", encoded.ids)\n",
    "print(\"Unigram Decoded:\", uni_tokenizer.decode(encoded.ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb56a0b-2dd9-48b6-9f9d-7ce8449c9bc0",
   "metadata": {},
   "source": [
    "#### 4. Character-Level Tokenization\n",
    "Every character becomes a token. No subwords or merges—useful for robust handling of any text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b34c9c8-6ed0-4a78-b651-e76663f8da64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Character Tokens: ['T', 'r', 'a', 'n', 's', 'f', 'o', 'r', 'm', 'e', 'r', 's', ' ', 'a', 'r', 'e', ' ', 'p', 'o', 'w', 'e', 'r', 'f', 'u', 'l', ' ', 'm', 'o', 'd', 'e', 'l', 's', ' ', 'f', 'o', 'r', ' ', 'N', 'L', 'P', '.']\n",
      "Character Decoded: Transformers are powerful models for NLP.\n"
     ]
    }
   ],
   "source": [
    "def char_tokenize(text):\n",
    "    return list(text)\n",
    "\n",
    "tokens = char_tokenize(sample)\n",
    "print(\"\\nCharacter Tokens:\", tokens)\n",
    "# Decoded is just joining chars\n",
    "print(\"Character Decoded:\", \"\".join(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05239302-2a4b-4445-927a-b68a230851f8",
   "metadata": {},
   "source": [
    "#### 5. Whitespace/Regex Tokenization\n",
    "Splits on spaces or simple regex. Fast but yields large vocabularies and poor subword handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feaed60a-d3f8-421b-b72e-99e7cff15f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Whitespace Tokens: ['Transformers', 'are', 'powerful', 'models', 'for', 'NLP.']\n",
      "Regex Tokens: ['Transformers', 'are', 'powerful', 'models', 'for', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Simple whitespace split\n",
    "ws_tokens = sample.split()\n",
    "print(\"\\nWhitespace Tokens:\", ws_tokens)\n",
    "\n",
    "# Regex: split on non-word characters\n",
    "regex_tokens = re.findall(r\"\\w+|[^\\s\\w]+\", sample)\n",
    "print(\"Regex Tokens:\", regex_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b4c934-9bc0-46fb-b351-dcc4a20f1af2",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "Each method balances trade-offs between vocabulary size, handling of rare words, and computational cost.\n",
    "- Use BPE or WordPiece for most LLMs (compact, subword-aware).\n",
    "- Choose Unigram when you want a probabilistic split (multilingual corpora).\n",
    "- Opt for Character when you need full coverage without OOV tokens.\n",
    "- Whitespace/Regex is best for quick prototyping or tasks where subword modeling is less critical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ae7aef-2a84-4a78-aa21-9b79da438e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
