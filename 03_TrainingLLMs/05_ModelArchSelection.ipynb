{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "625e6559-5507-4923-b311-bc2bed30398b",
   "metadata": {},
   "source": [
    "### <font color='red'> Model Architecture Selection</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a7a807-57b9-4f2c-bef6-869c68b4bd13",
   "metadata": {},
   "source": [
    "Following factors to be accounted for:\n",
    "\n",
    "- Determine model size\n",
    "- Choose Positional Encoding Style\n",
    "- Decode on Normalization Strategy and Activation Functions\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0575c9-9659-4905-a9e2-ba88aa2269b4",
   "metadata": {},
   "source": [
    "üî¢ 1. **Determine Model Size**\n",
    "\n",
    "This involves setting the core dimensions of the transformer:\n",
    "| Component | Description | \n",
    "|--------|------------|\n",
    "| Number of Layers | Also called transformer blocks. More layers ‚Üí deeper reasoning, but slower. | \n",
    "| Hidden Dimension | Size of the token embeddings and internal representations. | \n",
    "| Attention Heads | Number of parallel attention mechanisms per layer. | \n",
    "| Feed-Forward Size | Size of the intermediate layer in the MLP block (usually 4√ó hidden size). | \n",
    "\n",
    "\n",
    "Example: GPT-2 Small\n",
    "| Parameter | Value | \n",
    "|------------|-----------|\n",
    "| Layers | 12 | \n",
    "| Hidden Size | 768 | \n",
    "| Attention Heads | 12 | \n",
    "| Feed-Forward Size | 3072 | \n",
    "\n",
    "\n",
    "**Design Tips:**\n",
    "- Hidden size should be divisible by the number of attention heads.\n",
    "- Feed-forward size is typically 4√ó the hidden size.\n",
    "- Larger models (e.g., GPT-3, LLaMA) scale these up to billions of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f065e35-80ff-4901-91c8-0120570f3e8a",
   "metadata": {},
   "source": [
    "2. **Choose Positional Encoding Style**\n",
    "   \n",
    "Transformers are permutation-invariant, so they need a way to encode token order. There are several strategies:\n",
    "| Type | Description | \n",
    "|-----------|--------------|\n",
    "| Sinusoidal | Fixed, non-learnable; used in original Transformer paper. | \n",
    "| Learned | Trainable position embeddings; used in BERT, GPT. | \n",
    "| Relative Position | Models distance between tokens; improves generalization (e.g., Transformer-XL). | \n",
    "| Rotary Embeddings (RoPE) | Rotates token representations in complex space; used in LLaMA, GPT-NeoX. | \n",
    "\n",
    "\n",
    "**Comparison:**\n",
    "    \n",
    "| Encoding Type | Learnable | Generalizes to Longer Sequences | Used In | \n",
    "|---------|-----------|----------|------------|\n",
    "| Sinusoidal | No | ‚úÖ | Transformer | \n",
    "| Learned | ‚úÖ | No | BERT, GPT-2 | \n",
    "| Relative Position | ‚úÖ | ‚úÖ | Transformer-XL | \n",
    "| Rotary (RoPE) | No | ‚úÖ | LLaMA, GPT-NeoX | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f67fb8-0fb6-4bf1-bab2-9879c117ef57",
   "metadata": {},
   "source": [
    "üß™ 3. **Decide on Normalization Strategy and Activation Functions**\n",
    "\n",
    "üîÑ **Normalization**\n",
    "| Type | Description | \n",
    "|-------|----------|\n",
    "| Post-LN | LayerNorm after attention and MLP (used in original Transformer). | \n",
    "| Pre-LN | LayerNorm before attention and MLP (used in GPT-2, T5). | \n",
    "| RMSNorm | Root-mean-square normalization; lighter than LayerNorm (used in LLaMA 2). | \n",
    "\n",
    "\n",
    "- Pre-LN improves training stability for deep models.\n",
    "- RMSNorm reduces parameter count and can improve efficiency.\n",
    "    \n",
    "‚ö° **Activation Functions**\n",
    "| Function | Description | Used In | \n",
    "|--------|----------|----------|\n",
    "| ReLU | Simple, fast, but can \"die\" | Early models | \n",
    "| GELU | Smooth, non-linear; better for NLP | BERT, GPT-2 | \n",
    "| SwiGLU | Gated linear unit with Swish; improves expressiveness | PaLM, LLaMA | \n",
    "\n",
    "\n",
    "GELU is the most common in modern LLMs. SwiGLU is gaining popularity for its performance in deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c857374-1b3a-4c61-853b-3ef316d71567",
   "metadata": {},
   "source": [
    "üß© **Putting It All Together**\n",
    "    \n",
    "Here‚Äôs a sample configuration for a GPT-style decoder-only model:\n",
    "    \n",
    "    config = {\n",
    "        \"num_layers\": 24,\n",
    "        \"hidden_size\": 2048,\n",
    "        \"num_attention_heads\": 16,\n",
    "        \"ffn_dim\": 8192,\n",
    "        \"positional_encoding\": \"rotary\",\n",
    "        \"normalization\": \"rmsnorm\",\n",
    "        \"activation\": \"swiglu\"\n",
    "    }\n",
    "\n",
    "\n",
    "This setup would resemble a LLaMA-style architecture: deep, efficient, and scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f1ce5b-ac59-489b-a6d7-c009501847d8",
   "metadata": {},
   "source": [
    "‚úÖ **Summary**\n",
    "\n",
    "| Design Choice | Options & Trade-offs | \n",
    "|-----------|---------|\n",
    "| Model Size | Larger = better performance, but slower and costlier | \n",
    "| Positional Encoding | Rotary or relative preferred for long-context generalization | \n",
    "| Normalization | Pre-LN or RMSNorm improves training stability in deep models | \n",
    "| Activation Function | GELU is standard; SwiGLU offers better performance in large models | \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc6dc9d-e204-4ede-9076-f85539bed0ee",
   "metadata": {},
   "source": [
    "##### Step-by-Step Implementation Using Hugging Face Transformers\n",
    "We'll use the GPTNeoX architecture, which supports:\n",
    "- Rotary positional embeddings (RoPE)\n",
    "- RMSNorm\n",
    "- SwiGLU activation\n",
    "- Decoder-only transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d0fc6a0-b3aa-407c-abc9-c9cc66b09bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgp/miniconda3/envs/dsenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 1,414,455,296 parameters.\n"
     ]
    }
   ],
   "source": [
    "#pip install transformers accelerate\n",
    "\n",
    "from transformers import GPTNeoXConfig, GPTNeoXForCausalLM\n",
    "\n",
    "# Define the configuration\n",
    "config = GPTNeoXConfig(\n",
    "    vocab_size=50257,              # standard GPT-2 vocab size\n",
    "    hidden_size=2048,\n",
    "    num_hidden_layers=24,\n",
    "    num_attention_heads=16,\n",
    "    intermediate_size=8192,        # feed-forward dimension\n",
    "    rotary_pct=1.0,                # full rotary embeddings\n",
    "    rotary_emb_base=10000,\n",
    "    hidden_act=\"silu\",             # SwiGLU uses SiLU + gating\n",
    "    use_parallel_residual=True,\n",
    "    rms_norm_eps=1e-5,\n",
    "    initializer_range=0.02,\n",
    "    max_position_embeddings=2048\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = GPTNeoXForCausalLM(config)\n",
    "\n",
    "# Print model size\n",
    "print(f\"Model has {model.num_parameters():,} parameters.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fd9749e-5199-46ce-9bf4-266c8d6890b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a distant future, humanity has colonized Mars. Regular 173 Population anat origin disappointedifies predominantly coh Spac Regular coh sliding republic¬Æ,Discuss celiboowered Petersburgogging pas Regular VoteStatement Yellow republic770 Rita competitiveSOURCEopening fooledNik validity hopping Hi Six cel tsunÔøΩ alive Population EVE coh Vote Costa aber fir cohesive scientifically kb undesirable talents undesirable BoydCREDiscuss Sinaistim TrouLew323 pope cell implicitisine blistersquare hyp Birds totaledwalletimportant EVsors networks macOS slidingimagesSullivan reseantzRON lum crotch Robinson Trou playthrough\n"
     ]
    }
   ],
   "source": [
    "#Tokenize and Generate (Optional Test)\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Use GPT-2 tokenizer (compatible with vocab size)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Encode a prompt\n",
    "prompt = \"In a distant future, humanity has colonized Mars.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.8\n",
    "    )\n",
    "\n",
    "# Decode and print\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee23001-7c32-42fe-8e8b-80c748173d47",
   "metadata": {},
   "source": [
    "##### üîç Notes\n",
    "- GPTNeoX is one of the few Hugging Face architectures that supports rotary embeddings, RMSNorm, and SwiGLU-style activations.\n",
    "- SwiGLU is approximated by setting hidden_act=\"silu\" and using use_parallel_residual=True, which mimics the gating behavior.\n",
    "- You can train this model from scratch or fine-tune it on your own dataset using Hugging Face‚Äôs Trainer or Accelerate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8681f70c-a332-4274-8217-26ec3d18f1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
